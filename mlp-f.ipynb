{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:09.277446Z","iopub.execute_input":"2025-03-12T11:21:09.281726Z","iopub.status.idle":"2025-03-12T11:21:09.307628Z","shell.execute_reply.started":"2025-03-12T11:21:09.281574Z","shell.execute_reply":"2025-03-12T11:21:09.305789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nimport datetime\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.impute import SimpleImputer,KNNImputer\nfrom sklearn.metrics import classification_report\n# Import feature selection and dimensionality reduction methods\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.decomposition import TruncatedSVD\n# Import metric for evaluation\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:09.310917Z","iopub.execute_input":"2025-03-12T11:21:09.311338Z","iopub.status.idle":"2025-03-12T11:21:09.321987Z","shell.execute_reply.started":"2025-03-12T11:21:09.311302Z","shell.execute_reply":"2025-03-12T11:21:09.320533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/System-Threat-Forecaster/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:09.324883Z","iopub.execute_input":"2025-03-12T11:21:09.325304Z","iopub.status.idle":"2025-03-12T11:21:11.316258Z","shell.execute_reply.started":"2025-03-12T11:21:09.325272Z","shell.execute_reply":"2025-03-12T11:21:11.314989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/System-Threat-Forecaster/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:11.318505Z","iopub.execute_input":"2025-03-12T11:21:11.318949Z","iopub.status.idle":"2025-03-12T11:21:12.598329Z","shell.execute_reply.started":"2025-03-12T11:21:11.318907Z","shell.execute_reply":"2025-03-12T11:21:12.597319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:12.599402Z","iopub.execute_input":"2025-03-12T11:21:12.599710Z","iopub.status.idle":"2025-03-12T11:21:12.609976Z","shell.execute_reply.started":"2025-03-12T11:21:12.599684Z","shell.execute_reply":"2025-03-12T11:21:12.608878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # code for dropping the MachineID column as it is of no use for prediction\ndef drop_MachineID(df):\n    df=df.drop(['MachineID'],axis=1)\n    return df\ndf=drop_MachineID(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:12.610997Z","iopub.execute_input":"2025-03-12T11:21:12.611284Z","iopub.status.idle":"2025-03-12T11:21:12.659518Z","shell.execute_reply.started":"2025-03-12T11:21:12.611260Z","shell.execute_reply":"2025-03-12T11:21:12.658560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=['int64','float64'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:12.660653Z","iopub.execute_input":"2025-03-12T11:21:12.661004Z","iopub.status.idle":"2025-03-12T11:21:12.677896Z","shell.execute_reply.started":"2025-03-12T11:21:12.660978Z","shell.execute_reply":"2025-03-12T11:21:12.676615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:12.679052Z","iopub.execute_input":"2025-03-12T11:21:12.679453Z","iopub.status.idle":"2025-03-12T11:21:12.728241Z","shell.execute_reply.started":"2025-03-12T11:21:12.679385Z","shell.execute_reply":"2025-03-12T11:21:12.727222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (40,40))\nsns.heatmap(numerical_cols.corr(), annot =  True, cmap = 'coolwarm')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:12.731461Z","iopub.execute_input":"2025-03-12T11:21:12.731750Z","iopub.status.idle":"2025-03-12T11:21:18.994906Z","shell.execute_reply.started":"2025-03-12T11:21:12.731724Z","shell.execute_reply":"2025-03-12T11:21:18.993333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"below code checks with correlation that which are the most corrlated columns and out from that which one's are the one with lear corr with target that one we have to drop","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'numerical_cols' is your DataFrame with numerical columns and 'target' is your target variable\ncorr_matrix = numerical_cols.corr()\n\n# Find pairs of features with correlation greater than or equal to 0.8\nhigh_corr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):  # start from i+1 to avoid duplicate pairs\n        if abs(corr_matrix.iloc[i, j]) >= 0.8:  # Using abs() to catch both positive and negative correlations\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\n# Print the highly correlated pairs\nprint(\"Pairs of columns with correlation >= 0.8:\")\nfor pair in high_corr_pairs:\n    print(f\"{pair[0]} and {pair[1]}: {pair[2]:.2f}\")\n\n# Create a list of all columns involved in high correlation pairs\nto_drop = list(set([col for pair in high_corr_pairs for col in pair[:2]]))  # Using set() to remove duplicates\n\nprint(\"\\nAll columns involved in high correlations:\", to_drop)\n\n# Investigate each pair and determine which column to drop\nfinal_redundant_cols = []\nfor pair in high_corr_pairs:\n    col1, col2 = pair[0], pair[1]\n    if col1 in numerical_cols.columns and col2 in numerical_cols.columns:\n        print(f\"\\nInvestigating pair: {col1} and {col2}\")\n        corr_with_target = numerical_cols[[col1, col2]].corrwith(numerical_cols['target'])\n        print(\"Correlation with target:\")\n        print(corr_with_target)\n        print(\"\\nMissing values:\")\n        print(numerical_cols[[col1, col2]].isnull().sum())\n\n        # Determine which column to drop (the one with lower correlation with target)\n        if abs(corr_with_target[col1]) < abs(corr_with_target[col2]):\n            final_redundant_cols.append(col1)\n        else:\n            final_redundant_cols.append(col2)\n    else:\n        print(f\"\\nOne or both columns '{col1}' and '{col2}' not found in DataFrame.\")\n\n# Remove duplicates from final_redundant_cols\nfinal_redundant_cols = list(set(final_redundant_cols))\n\nprint(\"\\nFinal list of redundant columns (lower correlation with target in each pair):\")\nprint(final_redundant_cols)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:18.996802Z","iopub.execute_input":"2025-03-12T11:21:18.997726Z","iopub.status.idle":"2025-03-12T11:21:19.745102Z","shell.execute_reply.started":"2025-03-12T11:21:18.997691Z","shell.execute_reply":"2025-03-12T11:21:19.744148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def drop_redundant(df):\n    df=df.drop(['ProcessorManufacturerID', 'RealTimeProtectionState', 'OSBuildNumber', 'PrimaryDisplayResolutionVertical', 'OSUILocaleID'],axis=1)\n    return df\ndf=drop_redundant(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:19.746387Z","iopub.execute_input":"2025-03-12T11:21:19.746768Z","iopub.status.idle":"2025-03-12T11:21:19.780658Z","shell.execute_reply.started":"2025-03-12T11:21:19.746731Z","shell.execute_reply":"2025-03-12T11:21:19.779778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dropping columns with no variation or variablity\n","metadata":{}},{"cell_type":"code","source":"def drop_variablity(df):\n    columns_with_one_unique_value = df.loc[:, df.nunique() == 1]\n    df=df.drop(columns_with_one_unique_value,axis=1)\n    return df\n\ndf=drop_variablity(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:19.781622Z","iopub.execute_input":"2025-03-12T11:21:19.781878Z","iopub.status.idle":"2025-03-12T11:21:20.020632Z","shell.execute_reply.started":"2025-03-12T11:21:19.781855Z","shell.execute_reply":"2025-03-12T11:21:20.019680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.021566Z","iopub.execute_input":"2025-03-12T11:21:20.021813Z","iopub.status.idle":"2025-03-12T11:21:20.053167Z","shell.execute_reply.started":"2025-03-12T11:21:20.021791Z","shell.execute_reply":"2025-03-12T11:21:20.052141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plts for checking outliers\nsns.boxplot(df['TotalPhysicalRAMMB'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.054257Z","iopub.execute_input":"2025-03-12T11:21:20.054660Z","iopub.status.idle":"2025-03-12T11:21:20.219461Z","shell.execute_reply.started":"2025-03-12T11:21:20.054619Z","shell.execute_reply":"2025-03-12T11:21:20.218454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(df['SystemVolumeCapacityMB'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.220740Z","iopub.execute_input":"2025-03-12T11:21:20.221142Z","iopub.status.idle":"2025-03-12T11:21:20.366570Z","shell.execute_reply.started":"2025-03-12T11:21:20.221100Z","shell.execute_reply":"2025-03-12T11:21:20.365502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(df['PrimaryDisplayDiagonalInches'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.367704Z","iopub.execute_input":"2025-03-12T11:21:20.368093Z","iopub.status.idle":"2025-03-12T11:21:20.553403Z","shell.execute_reply.started":"2025-03-12T11:21:20.368043Z","shell.execute_reply":"2025-03-12T11:21:20.552221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(df['PrimaryDiskCapacityMB'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.554669Z","iopub.execute_input":"2025-03-12T11:21:20.555064Z","iopub.status.idle":"2025-03-12T11:21:20.701121Z","shell.execute_reply.started":"2025-03-12T11:21:20.555012Z","shell.execute_reply":"2025-03-12T11:21:20.700025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nimport numpy as np\n\ndef cap_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # Cap the outliers\n    df[column] = np.clip(df[column], lower_bound, upper_bound)\n    \n    return df\n\ndef handle_outliers(df):\n    columns_to_clean = ['TotalPhysicalRAMMB', 'PrimaryDisplayDiagonalInches', 'PrimaryDiskCapacityMB', 'SystemVolumeCapacityMB']\n    \n    for column in columns_to_clean:\n        if column in df.columns:\n            # Handle NaN values (you can choose to fill them or leave as is)\n            df[column] = df[column].fillna(df[column].median())  # Uncomment this line if you want to fill NaNs\n            df = cap_outliers(df, column)\n    \n    return df\n\n# Apply the function to your dataframe\ndf = handle_outliers(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.702005Z","iopub.execute_input":"2025-03-12T11:21:20.702254Z","iopub.status.idle":"2025-03-12T11:21:20.748796Z","shell.execute_reply.started":"2025-03-12T11:21:20.702232Z","shell.execute_reply":"2025-03-12T11:21:20.747942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.749687Z","iopub.execute_input":"2025-03-12T11:21:20.749928Z","iopub.status.idle":"2025-03-12T11:21:20.755531Z","shell.execute_reply.started":"2025-03-12T11:21:20.749907Z","shell.execute_reply":"2025-03-12T11:21:20.754449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"extract the date components","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef extract_date(df, date_as_col, date_os_col):\n    # Ensure the date columns are in datetime format\n    df[date_as_col] = pd.to_datetime(df[date_as_col], errors='coerce')\n    df[date_os_col] = pd.to_datetime(df[date_os_col], errors='coerce')\n\n    # Handle NaT values gracefully\n    df['DateAS_Days'] = df[date_as_col].dt.day.fillna(0).astype('Int64')  # Use Int64 to allow nulls\n    df['DateAS_Year'] = df[date_as_col].dt.year.fillna(0).astype('Int64')\n    df['DateAS_Month'] = df[date_as_col].dt.month.fillna(0).astype('Int64')\n\n    df['DateOS_Days'] = df[date_os_col].dt.day.fillna(0).astype('Int64')\n    df['DateOS_Year'] = df[date_os_col].dt.year.fillna(0).astype('Int64')\n    df['DateOS_Month'] = df[date_os_col].dt.month.fillna(0).astype('Int64')\n\n    # Optionally drop the original date columns\n    df = df.drop([date_as_col, date_os_col], axis=1)\n\n    return df\n\n# Example usage\n# Assuming 'df' is your DataFrame with 'DateAS' and 'DateOS' columns\ndf = extract_date(df, 'DateAS', 'DateOS')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.756980Z","iopub.execute_input":"2025-03-12T11:21:20.757369Z","iopub.status.idle":"2025-03-12T11:21:20.911686Z","shell.execute_reply.started":"2025-03-12T11:21:20.757321Z","shell.execute_reply":"2025-03-12T11:21:20.910661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" df['DateAS_Month']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.912559Z","iopub.execute_input":"2025-03-12T11:21:20.912798Z","iopub.status.idle":"2025-03-12T11:21:20.920377Z","shell.execute_reply.started":"2025-03-12T11:21:20.912777Z","shell.execute_reply":"2025-03-12T11:21:20.919249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['DateOS_Days']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.921282Z","iopub.execute_input":"2025-03-12T11:21:20.921579Z","iopub.status.idle":"2025-03-12T11:21:20.937658Z","shell.execute_reply.started":"2025-03-12T11:21:20.921553Z","shell.execute_reply":"2025-03-12T11:21:20.936614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"now firstly we will split the attribute to numerical and categorical and the we will extract different fields from the numerical part","metadata":{}},{"cell_type":"code","source":"def split_attribute(attr):\n    import re\n    # Extract numerical part (numbers and dots at the beginning of the string)\n    numerical_part = re.search(r'^[\\d.]+', attr)\n    numerical_part = numerical_part.group(0) if numerical_part else None\n    \n    # Extract categorical part (everything after the numerical part)\n    categorical_part = re.sub(r'^[\\d.]+', '', attr)\n    return numerical_part, categorical_part\n\n# Apply the function to split into two columns\ndf[['NumericalPart_OSBuildLab', 'CategoricalPart_OSBuildLab']] = df['OSBuildLab'].apply(lambda x: pd.Series(split_attribute(x)))\ndf=df.drop(['OSBuildLab','CategoricalPart_OSBuildLab'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:20.938756Z","iopub.execute_input":"2025-03-12T11:21:20.939094Z","iopub.status.idle":"2025-03-12T11:21:33.586316Z","shell.execute_reply.started":"2025-03-12T11:21:20.939060Z","shell.execute_reply":"2025-03-12T11:21:33.585319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[['NumericalPart_OSBuildLab', 'CategoricalPart_OSBuildLab']] = data['OSBuildLab'].apply(lambda x: pd.Series(split_attribute(x)))\ndata=data.drop(['OSBuildLab','CategoricalPart_OSBuildLab'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:33.587366Z","iopub.execute_input":"2025-03-12T11:21:33.587740Z","iopub.status.idle":"2025-03-12T11:21:44.792539Z","shell.execute_reply.started":"2025-03-12T11:21:33.587703Z","shell.execute_reply":"2025-03-12T11:21:44.791564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_numeric_version_column(df, column_name):\n\n    if column_name not in df.columns:\n        print(f\"Column '{column_name}' not found in DataFrame.\")\n        return df\n\n    try:\n        split_df = df[column_name].str.split('.', expand=True)\n\n        # Convert all resulting columns to integers. Fill NaN if conversion fails.\n        for col in split_df.columns:\n          split_df[col] = pd.to_numeric(split_df[col], errors='coerce').astype('Int64')\n\n        num_cols = split_df.shape[1]\n        new_cols = [f'{column_name}_part_{i+1}' for i in range(num_cols)]\n        df[new_cols] = split_df\n        df = df.drop(column_name,axis=1)\n        return df\n\n    except AttributeError as e:\n        print(f\"Error splitting column '{column_name}': {e}. Column is not of type string\")\n        return df\n    except ValueError as e:\n        print(f\"Error converting to numeric: {e}\")\n        return df\n    except TypeError as e:\n        print(f\"Error splitting column '{column_name}': {e}\")\n        return df\n\ndf=split_numeric_version_column(df,'NumericalPart_OSBuildLab')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:44.793521Z","iopub.execute_input":"2025-03-12T11:21:44.793870Z","iopub.status.idle":"2025-03-12T11:21:45.125515Z","shell.execute_reply.started":"2025-03-12T11:21:44.793834Z","shell.execute_reply":"2025-03-12T11:21:45.124700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=split_numeric_version_column(data,'NumericalPart_OSBuildLab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:45.126525Z","iopub.execute_input":"2025-03-12T11:21:45.126865Z","iopub.status.idle":"2025-03-12T11:21:45.626509Z","shell.execute_reply.started":"2025-03-12T11:21:45.126830Z","shell.execute_reply":"2025-03-12T11:21:45.625510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['NumericalPart_OSBuildLab_part_3']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:45.631638Z","iopub.execute_input":"2025-03-12T11:21:45.631934Z","iopub.status.idle":"2025-03-12T11:21:45.639562Z","shell.execute_reply.started":"2025-03-12T11:21:45.631907Z","shell.execute_reply":"2025-03-12T11:21:45.638349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"now we will extract the categorical version from the string","metadata":{}},{"cell_type":"code","source":"def split_version_column(df, column_name):\n    \"\"\"Splits a version string column into multiple numeric columns.\n\n    Handles mixed data types and non-string values gracefully.\n    \"\"\"\n    if column_name not in df.columns:\n        print(f\"Column '{column_name}' not found in DataFrame.\")\n        return df\n\n    try:\n        # Convert the column to string type first\n        df[column_name] = df[column_name].astype(str)\n        \n        split_df = df[column_name].str.split('.', expand=True)\n\n        # Convert all resulting columns to numeric. Use Int64 to handle potential NaNs.\n        for col in split_df.columns:\n            split_df[col] = pd.to_numeric(split_df[col], errors='coerce').astype('Int64')\n\n        num_cols = split_df.shape[1]\n        new_cols = [f'{column_name}_part_{i+1}' for i in range(num_cols)]\n        df[new_cols] = split_df\n        df = df.drop(columns=column_name)\n        return df\n\n    except AttributeError as e:\n        print(f\"Error splitting column '{column_name}': {e}. Column probably contains only NaN values\")\n        return df\n    except ValueError as e:\n        print(f\"Error during conversion to numeric in column '{column_name}': {e}\")\n        return df\n    except TypeError as e:\n        print(f\"Type error: {e}\")\n        return df\n    \ndef split_all(df):\n    df=split_version_column(df,'EngineVersion')\n    df=split_version_column(df,'AppVersion')\n    df=split_version_column(df,'SignatureVersion')\n    df=split_version_column(df,'OSVersion')\n    return df\n\n\ndf=split_all(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:45.641844Z","iopub.execute_input":"2025-03-12T11:21:45.642123Z","iopub.status.idle":"2025-03-12T11:21:47.688163Z","shell.execute_reply.started":"2025-03-12T11:21:45.642100Z","shell.execute_reply":"2025-03-12T11:21:47.687307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.689101Z","iopub.execute_input":"2025-03-12T11:21:47.689389Z","iopub.status.idle":"2025-03-12T11:21:47.696238Z","shell.execute_reply.started":"2025-03-12T11:21:47.689339Z","shell.execute_reply":"2025-03-12T11:21:47.695174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df['SignatureVersion_part_4']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.697457Z","iopub.execute_input":"2025-03-12T11:21:47.697841Z","iopub.status.idle":"2025-03-12T11:21:47.713634Z","shell.execute_reply.started":"2025-03-12T11:21:47.697800Z","shell.execute_reply":"2025-03-12T11:21:47.712534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y=df.target\ndf=df.drop(['target'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.714638Z","iopub.execute_input":"2025-03-12T11:21:47.714968Z","iopub.status.idle":"2025-03-12T11:21:47.755884Z","shell.execute_reply.started":"2025-03-12T11:21:47.714935Z","shell.execute_reply":"2025-03-12T11:21:47.754793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=['int64','float64']).columns\ncategorical_cols = df.select_dtypes(include=['object']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.756747Z","iopub.execute_input":"2025-03-12T11:21:47.757014Z","iopub.status.idle":"2025-03-12T11:21:47.813551Z","shell.execute_reply.started":"2025-03-12T11:21:47.756990Z","shell.execute_reply":"2025-03-12T11:21:47.812741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CLASSIFICATION using tranformers, pipelines, and different models","metadata":{}},{"cell_type":"code","source":"numerical_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numerical values\n        ('scaler', StandardScaler())                 # Scale numerical features\n    ])\n\n    # Categorical pipeline\ncategorical_pipeline = Pipeline([\n        ('imputer',SimpleImputer(strategy='most_frequent')),\n        ('hot',OneHotEncoder(handle_unknown='ignore',sparse_output=False))\n    ])\n\n    # Full preprocessing pipeline using ColumnTransformer\npreprocessor = ColumnTransformer(transformers=[\n        ('numerical', numerical_pipeline, numerical_cols),\n        ('categorical', categorical_pipeline, categorical_cols)\n    ], remainder='passthrough') #remainder passthrough keeps the columns that are not specified in the transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.814433Z","iopub.execute_input":"2025-03-12T11:21:47.814764Z","iopub.status.idle":"2025-03-12T11:21:47.820499Z","shell.execute_reply.started":"2025-03-12T11:21:47.814731Z","shell.execute_reply":"2025-03-12T11:21:47.819571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_data = preprocessor.fit_transform(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:47.821606Z","iopub.execute_input":"2025-03-12T11:21:47.821861Z","iopub.status.idle":"2025-03-12T11:21:50.726196Z","shell.execute_reply.started":"2025-03-12T11:21:47.821838Z","shell.execute_reply":"2025-03-12T11:21:50.725233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.727135Z","iopub.execute_input":"2025-03-12T11:21:50.727401Z","iopub.status.idle":"2025-03-12T11:21:50.732660Z","shell.execute_reply.started":"2025-03-12T11:21:50.727368Z","shell.execute_reply":"2025-03-12T11:21:50.731691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame\ndf = pd.DataFrame(transformed_data,columns=preprocessor.get_feature_names_out())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.733657Z","iopub.execute_input":"2025-03-12T11:21:50.733938Z","iopub.status.idle":"2025-03-12T11:21:50.753772Z","shell.execute_reply.started":"2025-03-12T11:21:50.733915Z","shell.execute_reply":"2025-03-12T11:21:50.752543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.754974Z","iopub.execute_input":"2025-03-12T11:21:50.755359Z","iopub.status.idle":"2025-03-12T11:21:50.769785Z","shell.execute_reply.started":"2025-03-12T11:21:50.755317Z","shell.execute_reply":"2025-03-12T11:21:50.768738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\nprint(X_train.shape, y_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.770753Z","iopub.execute_input":"2025-03-12T11:21:50.771119Z","iopub.status.idle":"2025-03-12T11:21:50.939837Z","shell.execute_reply.started":"2025-03-12T11:21:50.771071Z","shell.execute_reply":"2025-03-12T11:21:50.938794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model=LogisticRegression()\n# model.fit(X_train,y_train)\n# y_pred=model.predict(X_train)\n# print(classification_report(y_train ,y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.940674Z","iopub.execute_input":"2025-03-12T11:21:50.940949Z","iopub.status.idle":"2025-03-12T11:21:50.944625Z","shell.execute_reply.started":"2025-03-12T11:21:50.940916Z","shell.execute_reply":"2025-03-12T11:21:50.943525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_pred=model.predict(X_test)\n# print(classification_report(y_test ,y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.945693Z","iopub.execute_input":"2025-03-12T11:21:50.946100Z","iopub.status.idle":"2025-03-12T11:21:50.960476Z","shell.execute_reply.started":"2025-03-12T11:21:50.946007Z","shell.execute_reply":"2025-03-12T11:21:50.959535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sgd_lasso = SGDClassifier(loss='hinge', penalty='l1', random_state=42, max_iter=1000, tol=1e-3) #loss='hinge' for linear SVM\n\n# # Train the classifier\n# sgd_lasso.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = sgd_lasso.predict(X_test)\n# print(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.961430Z","iopub.execute_input":"2025-03-12T11:21:50.961702Z","iopub.status.idle":"2025-03-12T11:21:50.979196Z","shell.execute_reply.started":"2025-03-12T11:21:50.961677Z","shell.execute_reply":"2025-03-12T11:21:50.977878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = DecisionTreeClassifier(random_state=42, max_depth=8) #max_depth to prevent overfitting\n# # Train the classifier\n# model.fit(X_train, y_train)\n# y_pred=model.predict(X_train)\n# print(classification_report(y_train, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.980123Z","iopub.execute_input":"2025-03-12T11:21:50.980468Z","iopub.status.idle":"2025-03-12T11:21:50.995896Z","shell.execute_reply.started":"2025-03-12T11:21:50.980427Z","shell.execute_reply":"2025-03-12T11:21:50.994832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_pred = model.predict(X_test)\n\n# # Evaluate the classifier\n# print(classification_report(y_test, y_pred))\n\n# from sklearn.ensemble import RandomForestClassifier\n# model=RandomForestClassifier(max_depth=8,n_estimators=20)\n# model.fit(X_train,y_train)\n# y_pred=model.predict(X_test)\n# print(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:50.996832Z","iopub.execute_input":"2025-03-12T11:21:50.997238Z","iopub.status.idle":"2025-03-12T11:21:51.011089Z","shell.execute_reply.started":"2025-03-12T11:21:50.997199Z","shell.execute_reply":"2025-03-12T11:21:51.010174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.linear_model import Perceptron\n# perceptron_clf = Perceptron(\n#     penalty=None,          # No regularization (L1 or L2)\n#     alpha=0.1,          # Regularization strength (if penalty is set)\n#     max_iter=10000,         # Maximum number of iterations (epochs)\n#     tol=1e-3,              # Stopping criterion tolerance\n#     random_state=42,\n#     l1_ratio=1\n# )\n\n# # Step 4: Train the Perceptron model\n# for i in range(10):\n#     perceptron_clf.fit(X_train, y_train)\n\n# # Step 5: Make predictions\n# y_pred = perceptron_clf.predict(X_test)\n\n# print(\"\\nClassification Report:\")\n# print(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:51.011976Z","iopub.execute_input":"2025-03-12T11:21:51.012228Z","iopub.status.idle":"2025-03-12T11:21:51.033155Z","shell.execute_reply.started":"2025-03-12T11:21:51.012206Z","shell.execute_reply":"2025-03-12T11:21:51.032146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(x_train, x_test, y_train, y_test):\n    \n    # A base model is defined\n    base_model = LogisticRegression(max_iter=300)\n    \n    # Define feature engineering pipelines\n    methods = {\n        \"Base_model\": Pipeline([\n            ('model', base_model)\n        ], verbose = True),\n        \n        \"Select_K_Best\": Pipeline([\n            ('feature_engineering', SelectKBest(score_func=f_classif, k=70)),\n            ('model', base_model)\n        ], verbose = True),\n        \n        \"Truncated_SVD\": Pipeline([\n            ('feature_engineering', TruncatedSVD(n_components=70)),\n            ('model', base_model)\n        ], verbose = True)\n        \n     }\n\n    # A variable to store the scores is set\n    fe_scores = {}\n\n    for name, method in methods.items():             # Iterate through each model in the 'methods' dictionary\n        \n        print(f\"\\n-----Processing with {name}-----\")\n        \n        method.fit(x_train, y_train)                 # Fitting with various methods\n        preds = method.predict(x_test)               # Predicting with sklearn's .predict method\n        \n        accuracy = accuracy_score(y_test, preds)     # Finding accuracy of prediction\n        fe_scores[name] = accuracy                   # Storing accuracy in 'fe_scores'\n        \n        print(f\"{name} Accuracy: {accuracy:.4f}\\n\")\n        print(f\"-----Process done : {name} -----\\n\")\n    \n    return fe_scores ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:51.034327Z","iopub.execute_input":"2025-03-12T11:21:51.034710Z","iopub.status.idle":"2025-03-12T11:21:51.051472Z","shell.execute_reply.started":"2025-03-12T11:21:51.034673Z","shell.execute_reply":"2025-03-12T11:21:51.050388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_engineering(X_train,X_test,y_train,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:21:51.052498Z","iopub.execute_input":"2025-03-12T11:21:51.052852Z","iopub.status.idle":"2025-03-12T11:22:07.498890Z","shell.execute_reply.started":"2025-03-12T11:21:51.052815Z","shell.execute_reply":"2025-03-12T11:22:07.497849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingClassifier  # For Gradient Boosting Classifier\n\n# def model_training():\n    \n#     # Create a dictionary of models to be trained\n#     models = {\n#         'Logistic_Regression': LogisticRegression(max_iter=300),         # Logistic Regression model\n#         'Gradient_Boosting_Classifier': GradientBoostingClassifier()     # Gradient Boosting Classifier model\n#     }\n\n#     m_scores = {}       # Dictionary to store accuracy scores of each model\n#     predictions = {}    # Dictionary to store predictions of each model\n\n#     # Iterate over each model in the models dictionary\n#     for name, model in models.items():\n#         print(f\"-----------------------------\")\n#         print(f\"Training with {name} model...\")\n        \n#         # Create a pipeline with preprocessing and the current model\n#         model_pipeline = Pipeline([\n#             ('model', model)                   # Apply the model\n#         ])\n        \n#         # Fit the pipeline on training data\n#         model_pipeline.fit(X_train, y_train)\n        \n#         # Predict on test data\n#         y_pred = model_pipeline.predict(X_test)\n        \n#         # Calculate accuracy score of the predictions\n#         accuracy = accuracy_score(y_test, y_pred)\n        \n#         # Store the accuracy score and predictions in the dictionaries\n#         m_scores[name] = accuracy\n#         predictions[name] = y_pred\n        \n#         print(f\"{name} Accuracy: {accuracy:.4f}\\n\")\n#         print(f\"Model training done for: {name}\")\n      \n#     # Return the accuracy scores and predictions\n#     return m_scores, predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.499995Z","iopub.execute_input":"2025-03-12T11:22:07.500384Z","iopub.status.idle":"2025-03-12T11:22:07.505317Z","shell.execute_reply.started":"2025-03-12T11:22:07.500334Z","shell.execute_reply":"2025-03-12T11:22:07.504108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pca=PCA(n_components=80)\n# X_train_=pca.fit_transform(X_train)\n# X_test_=pca.transform(X_test)\n\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.506492Z","iopub.execute_input":"2025-03-12T11:22:07.506910Z","iopub.status.idle":"2025-03-12T11:22:07.523923Z","shell.execute_reply.started":"2025-03-12T11:22:07.506872Z","shell.execute_reply":"2025-03-12T11:22:07.522837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# threshold = .98\n# optimal_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= threshold) + 1\n# print(f\"Number of components to capture {threshold*100}% variance: {optimal_components}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.524970Z","iopub.execute_input":"2025-03-12T11:22:07.525298Z","iopub.status.idle":"2025-03-12T11:22:07.540017Z","shell.execute_reply.started":"2025-03-12T11:22:07.525263Z","shell.execute_reply":"2025-03-12T11:22:07.538902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# m_scores,predictions=model_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.541064Z","iopub.execute_input":"2025-03-12T11:22:07.541467Z","iopub.status.idle":"2025-03-12T11:22:07.555745Z","shell.execute_reply.started":"2025-03-12T11:22:07.541426Z","shell.execute_reply":"2025-03-12T11:22:07.554662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Y_pred=predictions['Gradient_Boosting_Classifier']\n# print(classification_report(Y_pred,y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.556791Z","iopub.execute_input":"2025-03-12T11:22:07.557143Z","iopub.status.idle":"2025-03-12T11:22:07.574532Z","shell.execute_reply.started":"2025-03-12T11:22:07.557103Z","shell.execute_reply":"2025-03-12T11:22:07.573501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model=GradientBoostingClassifier()\n# model.fit(X_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.575696Z","iopub.execute_input":"2025-03-12T11:22:07.576032Z","iopub.status.idle":"2025-03-12T11:22:07.590445Z","shell.execute_reply.started":"2025-03-12T11:22:07.575996Z","shell.execute_reply":"2025-03-12T11:22:07.589320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.591542Z","iopub.execute_input":"2025-03-12T11:22:07.591936Z","iopub.status.idle":"2025-03-12T11:22:07.610676Z","shell.execute_reply.started":"2025-03-12T11:22:07.591907Z","shell.execute_reply":"2025-03-12T11:22:07.609590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBClassifier(\n    colsample_bytree=0.9,\n    gamma=0,\n    reg_alpha=0,\n    reg_lambda=2,\n    learning_rate=.1,\n    max_depth=4,            # Note: max_depth should be an integer (not 0.3)\n    n_estimators=550,        # Correct parameter name is n_estimators\n    subsample=0.8,\n    use_label_encoder=False,  # Suppress label encoder warnings\n    eval_metric='logloss'     # Set evaluation metric to avoid warnings\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.611657Z","iopub.execute_input":"2025-03-12T11:22:07.611976Z","iopub.status.idle":"2025-03-12T11:22:07.632093Z","shell.execute_reply.started":"2025-03-12T11:22:07.611935Z","shell.execute_reply":"2025-03-12T11:22:07.630858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = XGBClassifier()\n# param_grid = {\n#     'n_estimators': [500, 800],  # Reduce range\n#     'max_depth': [2,3],        # Smaller range\n#     'learning_rate': [0.1],\n#     'subsample': [0.8],         # Fix to one value\n#     'colsample_bytree': [0.8],\n#     'gamma': [0],          # Limit values\n#    'reg_alpha': [0, 0.01, 0.1],          # L1 regularization\n#     'reg_lambda': [1, 2, 5] \n# }\n\n# #Initialize GridSearchCV\n# grid_search = GridSearchCV(\n#     estimator=model,\n#     param_grid=param_grid,\n#     cv=3,                                # 3-fold cross-validation\n#     n_jobs=-1,                           # Use all available cores\n#     verbose=2                            # Progress updates\n# )\n\n# if X_train is not None and y_train is not None:\n#     print(\"Starting GridSearchCV for XGBClassifier...\")\n\n#     # Fit GridSearchCV\n#     grid_search.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.633377Z","iopub.execute_input":"2025-03-12T11:22:07.633708Z","iopub.status.idle":"2025-03-12T11:22:07.638455Z","shell.execute_reply.started":"2025-03-12T11:22:07.633678Z","shell.execute_reply":"2025-03-12T11:22:07.637283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the best parameters\n# print(\"Best Parameters:\", grid_search.best_params_)\n\n# # Display the best cross-validation score\n# print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n\n# # Display the best estimator (the model with the best parameters)\n# print(\"Best Estimator:\", grid_search.best_estimator_)\n\n# # Access the detailed cross-validation results\n# cv_results = grid_search.cv_results_\n\n# # Show the keys in cv_results_ for reference\n# print(\"Keys in cv_results_:\", cv_results.keys())\n\n# # Example: Display mean test scores for each parameter combination\n# mean_test_scores = cv_results['mean_test_score']\n# print(\"Mean Test Scores for Each Parameter Combination:\", mean_test_scores)\n\n# # Combine parameter combinations and scores in a DataFrame (optional)\n# results_df = pd.DataFrame(cv_results)\n# print(results_df[['params', 'mean_test_score', 'std_test_score']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.639502Z","iopub.execute_input":"2025-03-12T11:22:07.639868Z","iopub.status.idle":"2025-03-12T11:22:07.657193Z","shell.execute_reply.started":"2025-03-12T11:22:07.639832Z","shell.execute_reply":"2025-03-12T11:22:07.656062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model=grid_search.best_estimator_\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_train)\nprint(classification_report(y_pred,y_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:07.658108Z","iopub.execute_input":"2025-03-12T11:22:07.658467Z","iopub.status.idle":"2025-03-12T11:22:25.239508Z","shell.execute_reply.started":"2025-03-12T11:22:07.658403Z","shell.execute_reply":"2025-03-12T11:22:25.238426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:25.240511Z","iopub.execute_input":"2025-03-12T11:22:25.240893Z","iopub.status.idle":"2025-03-12T11:22:25.427876Z","shell.execute_reply.started":"2025-03-12T11:22:25.240846Z","shell.execute_reply":"2025-03-12T11:22:25.426631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PERFORM ALL THE SAME OPERATIONS ON THE TEST DATA AS PERFORMED ON THE TTRAIN DATA","metadata":{}},{"cell_type":"code","source":"test_data=pd.read_csv('/kaggle/input/System-Threat-Forecaster/test.csv')\ntest_data.describe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:25.429080Z","iopub.execute_input":"2025-03-12T11:22:25.429378Z","iopub.status.idle":"2025-03-12T11:22:25.582013Z","shell.execute_reply.started":"2025-03-12T11:22:25.429344Z","shell.execute_reply":"2025-03-12T11:22:25.580841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_data=drop_MachineID(test_data)\ntest_data=drop_redundant(test_data)\ntest_data=drop_variablity(test_data)\n\ntest_data=handle_outliers(test_data)\ntest_data=extract_date(test_data,'DateAS','DateOS')\ntest_data[['NumericalPart_OSBuildLab', 'CategoricalPart_OSBuildLab']] = test_data['OSBuildLab'].apply(lambda x: pd.Series(split_attribute(x)))\ntest_data=test_data.drop(['OSBuildLab','CategoricalPart_OSBuildLab'],axis=1)\ntest_data=split_numeric_version_column(test_data,'NumericalPart_OSBuildLab')\ntest_data=split_all(test_data)\ntransformed_data = preprocessor.transform(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:25.582930Z","iopub.execute_input":"2025-03-12T11:22:25.583223Z","iopub.status.idle":"2025-03-12T11:22:27.206637Z","shell.execute_reply.started":"2025-03-12T11:22:25.583194Z","shell.execute_reply":"2025-03-12T11:22:27.205728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = pd.DataFrame(transformed_data,columns=preprocessor.get_feature_names_out())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:27.207557Z","iopub.execute_input":"2025-03-12T11:22:27.207817Z","iopub.status.idle":"2025-03-12T11:22:27.214219Z","shell.execute_reply.started":"2025-03-12T11:22:27.207795Z","shell.execute_reply":"2025-03-12T11:22:27.213048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred=model.predict(test_data)\nsubmission=pd.DataFrame({'id':range(0,test_data.shape[0]),\n                        'target':y_pred})\n\nsubmission.to_csv('submission.csv',index=False)\nprint(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:27.215307Z","iopub.execute_input":"2025-03-12T11:22:27.215701Z","iopub.status.idle":"2025-03-12T11:22:27.329891Z","shell.execute_reply.started":"2025-03-12T11:22:27.215664Z","shell.execute_reply":"2025-03-12T11:22:27.328847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T11:22:27.330791Z","iopub.execute_input":"2025-03-12T11:22:27.331263Z","iopub.status.idle":"2025-03-12T11:22:27.337333Z","shell.execute_reply.started":"2025-03-12T11:22:27.331224Z","shell.execute_reply":"2025-03-12T11:22:27.336283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}